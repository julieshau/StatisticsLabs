{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "**Problem 5b (Testing a sampler).** In this problem we will attempt to check whether the sampler we created in **Problem 2c** works correctly. To this end we will use a chi-squared goodness-of-fit test. This test works as follows:\n",
    " * Let $p_1,\\ldots,p_d$ be the date frequencies as in the text file, scaled down to sum up to 1.\n",
    " * Use the sampler to generate a sample of dates. Let $c_1,\\ldots,c_d$ be the observed counts, and let $f_i=Np_i$ be the expected counts, where $N$ is the sample size.\n",
    " * Compute the test statistic $$S = \\sum_{i=1}^d \\frac{\\left(c_i-f_i\\right)^2}{f_i}.$$\n",
    " * Our base assumption (the null hypothesis) $H_0$ is that our sampler works correctly. If $H_0$ is true AND if the expected count for each bucket is large enough, then $S$ has (approximately) a $\\chi^2$ distribution with $d-1$ degrees of freedom.\n",
    " * Look up how likely is getting an $S$ value as large as the one you obtained if it has that distribution, i.e. the $p$-value. To do this use **scipy.stats.chi2.cdf**. If this value turns out smaller than the assumed threshold, e.g. $0.05$, we reject $H_0$. Otherwise we do not (we support $H_0$), but this does not mean $H_0$ is proved!\n",
    " * We mentioned earlier that expected counts for the buckets need to be large enough. \"Large enough\" assumption here is used to guarantee that $c_i$ are distributed approximately normally. Typically one requires that all counts are at least $5$. This is not the case in our problem (unless we take a huge sample) because of the errors in the data. The typical approach is to glue several buckets into one but this does not help in our case. Instead, ignore the erroneous dates when computing $c_i$ and $f_i$ and run the test again (on the same sample!). Remember to use a different number of degrees of freedom. Compare the results.\n",
    " * Perform the same test using **scipy.stats.chisquare** and compare the results."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " *********** Iteration number  1 ***********\n",
      "Data without filter\n",
      "Computed test statistic 455.2\n",
      "chi2.cdf= 0.0018183572479134602\n",
      "chisquare= Power_divergenceResult(statistic=455.2, pvalue=0.0018183572479134778)\n",
      "Filtered data\n",
      "Computed test statistic 422.9419354838709\n",
      "chi2.cdf= 0.01946676926922808\n",
      "chisquare= Power_divergenceResult(statistic=422.9419354838709, pvalue=0.019466769269228114)\n",
      " *********** Iteration number  2 ***********\n",
      "Data without filter\n",
      "Computed test statistic 401.632\n",
      "chi2.cdf= 0.13150398818511\n",
      "chisquare= Power_divergenceResult(statistic=401.632, pvalue=0.13150398818511005)\n",
      "Filtered data\n",
      "Computed test statistic 369.3739354838709\n",
      "chi2.cdf= 0.4262625317875909\n",
      "chisquare= Power_divergenceResult(statistic=369.3739354838709, pvalue=0.42626253178759094)\n",
      " *********** Iteration number  3 ***********\n",
      "Data without filter\n",
      "Computed test statistic 400.51599999999996\n",
      "chi2.cdf= 0.13997893369304282\n",
      "chisquare= Power_divergenceResult(statistic=400.51599999999996, pvalue=0.1399789336930429)\n",
      "Filtered data\n",
      "Computed test statistic 368.25793548387094\n",
      "chi2.cdf= 0.4424011943806496\n",
      "chisquare= Power_divergenceResult(statistic=368.25793548387094, pvalue=0.4424011943806496)\n",
      " *********** Iteration number  4 ***********\n",
      "Data without filter\n",
      "Computed test statistic 419.116\n",
      "chi2.cdf= 0.0427565116005193\n",
      "chisquare= Power_divergenceResult(statistic=419.116, pvalue=0.0427565116005193)\n",
      "Filtered data\n",
      "Computed test statistic 386.85793548387096\n",
      "chi2.cdf= 0.2066934086222194\n",
      "chisquare= Power_divergenceResult(statistic=386.85793548387096, pvalue=0.20669340862221933)\n",
      " *********** Iteration number  5 ***********\n",
      "Data without filter\n",
      "Computed test statistic 437.34399999999994\n",
      "chi2.cdf= 0.009955999550429029\n",
      "chisquare= Power_divergenceResult(statistic=437.34399999999994, pvalue=0.009955999550429057)\n",
      "Filtered data\n",
      "Computed test statistic 405.0859354838709\n",
      "chi2.cdf= 0.07255445662078952\n",
      "chisquare= Power_divergenceResult(statistic=405.0859354838709, pvalue=0.07255445662078962)\n",
      " *********** Iteration number  6 ***********\n",
      "Data without filter\n",
      "Computed test statistic 413.53599999999994\n",
      "chi2.cdf= 0.06303095844299167\n",
      "chisquare= Power_divergenceResult(statistic=413.53599999999994, pvalue=0.06303095844299168)\n",
      "Filtered data\n",
      "Computed test statistic 381.2779354838709\n",
      "chi2.cdf= 0.2681787404654299\n",
      "chisquare= Power_divergenceResult(statistic=381.2779354838709, pvalue=0.26817874046542994)\n",
      " *********** Iteration number  7 ***********\n",
      "Data without filter\n",
      "Computed test statistic 387.12399999999997\n",
      "chi2.cdf= 0.2716082352788258\n",
      "chisquare= Power_divergenceResult(statistic=387.12399999999997, pvalue=0.2716082352788258)\n",
      "Filtered data\n",
      "Computed test statistic 354.8659354838709\n",
      "chi2.cdf= 0.6382825723186287\n",
      "chisquare= Power_divergenceResult(statistic=354.8659354838709, pvalue=0.6382825723186287)\n",
      " *********** Iteration number  8 ***********\n",
      "Data without filter\n",
      "Computed test statistic 377.08\n",
      "chi2.cdf= 0.4026707898178221\n",
      "chisquare= Power_divergenceResult(statistic=377.08, pvalue=0.40267078981782206)\n",
      "Filtered data\n",
      "Computed test statistic 344.82193548387096\n",
      "chi2.cdf= 0.76921194736677\n",
      "chisquare= Power_divergenceResult(statistic=344.82193548387096, pvalue=0.76921194736677)\n",
      " *********** Iteration number  9 ***********\n",
      "Data without filter\n",
      "Computed test statistic 435.856\n",
      "chi2.cdf= 0.011334542676206816\n",
      "chisquare= Power_divergenceResult(statistic=435.856, pvalue=0.011334542676206861)\n",
      "Filtered data\n",
      "Computed test statistic 403.59793548387097\n",
      "chi2.cdf= 0.07992450650577898\n",
      "chisquare= Power_divergenceResult(statistic=403.59793548387097, pvalue=0.07992450650577898)\n",
      " *********** Iteration number  10 ***********\n",
      "Data without filter\n",
      "Computed test statistic 416.14\n",
      "chi2.cdf= 0.052769741609610565\n",
      "chisquare= Power_divergenceResult(statistic=416.14, pvalue=0.052769741609610614)\n",
      "Filtered data\n",
      "Computed test statistic 383.8819354838709\n",
      "chi2.cdf= 0.23831752277906815\n",
      "chisquare= Power_divergenceResult(statistic=383.8819354838709, pvalue=0.23831752277906815)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.stats\n",
    "\n",
    "N = 2000\n",
    "ITER = 10\n",
    "########################################################################\n",
    "def align(table):\n",
    "    birth_all = np.sum(table)\n",
    "    size = table.size\n",
    "    extra = birth_all % size\n",
    "    extra = size - extra\n",
    "    table[:extra] += 1\n",
    "    birth_all += extra\n",
    "    table_height = birth_all/size\n",
    "    return table_height\n",
    "\n",
    "def transfer(table, barrier, donor, height):\n",
    "    max_t = np.max(table)\n",
    "    min_t = np.min(table)\n",
    "    while max_t != min_t:\n",
    "        transfer_amount = height - min_t\n",
    "        index_max = np.argmax(table)\n",
    "        index_min = np.argmin(table)\n",
    "        table[index_max] -= transfer_amount\n",
    "        table[index_min] += transfer_amount\n",
    "        donor[index_min] = index_max\n",
    "        barrier[index_min] = index_min + (min_t/height)\n",
    "        max_t = np.max(table)\n",
    "        min_t = np.min(table)\n",
    "    for i in range(table.size):\n",
    "        if barrier[i] == 0:\n",
    "            barrier[i] = i + 1\n",
    "\n",
    "def choose_day(barrier, donor):\n",
    "    r = np.random.rand(N) * donor.size\n",
    "    j = np.floor(r).astype('int64')\n",
    "    #if r < barrier(floor(r)) return floor(r)\n",
    "    #else return donor(floor(r))\n",
    "    return np.concatenate((j[r < barrier[j]], donor[j[r >= barrier[j]]]), axis=None)\n",
    "#########################################################################\n",
    "\n",
    "Data = np.loadtxt('us_births_69_88.csv', skiprows=1, delimiter=',', dtype=np.int64)\n",
    "table = Data[:,2]\n",
    "error = np.where(table < 1000)\n",
    "height = align(table)\n",
    "donor = np.zeros(table.size, dtype=np.int64)\n",
    "barrier = np.zeros(table.size, dtype=np.double)\n",
    "transfer(table, barrier, donor, height)\n",
    "########################################################################\n",
    "def filter_data(data):\n",
    "    #data[(tab < 1000)]\n",
    "    return np.delete(data, error) #error is defined after reading us_births\n",
    "\n",
    "def count_s(sample, exp):\n",
    "    return np.sum(np.power(sample - exp, 2)/exp)\n",
    "\n",
    "def print_res(s, sample, exp):\n",
    "        print(\"Computed test statistic\", s)\n",
    "        print(\"chi2.cdf=\", 1 - scipy.stats.chi2.cdf(s, sample.size - 1))\n",
    "        print(\"chisquare=\" , scipy.stats.chisquare(sample, f_exp=exp))\n",
    "\n",
    "def results(iter_number):\n",
    "    for i in range(iter_number):\n",
    "        print(\" *********** Iteration number \", i+1, \"***********\")\n",
    "        print(\"Data without filter\")\n",
    "        exp = table/np.sum(table) * N #expected counts\n",
    "        sample = choose_day(barrier, donor) #sampling with sampler from 2c\n",
    "        sample = np.bincount(sample) #observed counts\n",
    "        #add zeros to end if max(sample) < amount od days(last days aren't in sample)\n",
    "        sample = np.pad(sample, (0, exp.size - sample.size), 'constant')\n",
    "        s = count_s(sample, exp) #computation of the test statistic\n",
    "        print_res(s, sample, exp)\n",
    "        print(\"Filtered data\")\n",
    "        #ignoring the erroneous dates\n",
    "        filter_exp = filter_data(exp)\n",
    "        filter_sample = filter_data(sample)\n",
    "        s = count_s(filter_sample, filter_exp)\n",
    "        print_res(s, filter_sample, filter_exp)\n",
    "\n",
    "results(ITER)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}